# 역전파 이론
'''
## 역전파
역전파 = backpropagation, 오차역전파법
역전파를 이용하면 미분을 효율적으로 계산할 수 있고 결괏값의 오차도 더 적습니다.

## 연쇄 법칙
역전파를 이해하는 열쇠는 연쇄 법칙(chain rule)입니다. chain은 여러 함수를 사슬처럼 연결하여 사용하는 모습을 빗댄 것입니다.

연쇄 법칙에 따르면 합성 함수(여러 함수가 연결된 함수)의 미분은 구성 함수 각각을 미분한 후 곱한 것과 같습니다.
즉 합성 함수의 미분은 각 함수의 국소적인 미분들로 분해할 수 있습니다.
참고로 (dy/dy)는 '자신'에 대한 미분이라 값이 항상 1입니다.

## 역전파 원리 도출
합성 함수의 미분은 구성 함수들의 미분의 곱으로 분해할 수 있으며, 이때 순서는 상관 없습니다.
그러니 출력에서 입력 방향(역방향으로)으로 계산을 해볼 수 있습니다.
dy/dx = ((((dy/dy)*(dy/db))*(db/da))*(da/dx))

아러한 도함수 식을 함수 노드 하나로 그릴 수 있습니다. 이렇게 미분값이 전파되는 흐름이 명확해집니다.
y'의 각 변수에 대한 미분값이, 즉 변수 y,b,a,x에 대한 미분값이 오른쪽에서 왼쪽으로 전파됩니다.
이것이 역전파입니다.

여기서 중요한 점은 전파되는 데이터는 모두 'y의 미분값'이라는 것입니다.
구체적으로는 dy/dy, dy/db, dy/da, dy/dx처럼 모두 'y의 oo에 대한 미분값'이 전파되고 있습니다.

계산순서를 역방향으로 정한 이유가 y의 미분값을 전파하기 위해서이며, y를 '중요 요소'로 대우하기 때문입니다.

머신러닝은 주로 대량의 매개변수를 입력받아서 마지막에 손실 함수(loss function)를 거쳐 출력을 내는 형태로 진행됩니다.
손실 함수의 출력은 (많은 경우) 단일한 스칼라값이며, 이 값이 '중요 요소'입니다.
즉, 손실 함수의 각 매개변수에 대한 미분을 계산해야 합니다.
이런 경우 미분값을 출력에서 입력 방향으로 전파하면 한 번의 전파만으로 모든 매개변수에 대한 미분을 계산할 수 있습니다.

## 계산 그래프로 살펴보기
순전파와 역전파
순전파: x->A->a->B->b->C->y
역전파: dy/dx<-A'(x)<-dy/da<-B'(a)<-dy/db<-C'(b)<-dy/dy

통상 계산(순전파), 미분값을 구하기 위한 계산(역전파)
함수 노드 C'(b)에서, C'(b)를 구하려면 b값이 필요합니다. 
즉 역전파 시에는 순전파 시 이용한 데이터가 필요합니다.
역전파를 구현하려면 먼저 순전파를 하고, 이때 각 함수가 입력 변수(x,a,b)의 값을 기억해두어야 합니다.
'''
